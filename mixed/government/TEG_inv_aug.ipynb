{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/guest/Desktop/projects/third-experiments/domain_adaptation_project/mixed/government', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages', '/tmp/tmp_89nsr0_', '/home/guest/Desktop/projects/third-experiments/domain_adaptation_project/modules']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 17:35:46.899003: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-12 17:35:47.113314: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 17:35:47.824471: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./text-files/\n",
      "./hp-model-\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Initialize the console\n",
    "\n",
    "# Step 2: Import necessary libraries\n",
    "from typing import Optional, Dict, Any\n",
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rich.traceback import install\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoConfig\n",
    "from adapters import AutoAdapterModel, AdapterConfig\n",
    "from adapters.composition import Stack\n",
    "from pytorch_lightning import seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint,EarlyStopping\n",
    "import torchmetrics\n",
    "\n",
    "install(show_locals=True)\n",
    "\n",
    "from setup import setup_src_path\n",
    "print(setup_src_path())\n",
    "import data.processed as processed\n",
    "import config.config as config\n",
    "import utils.setup as setup\n",
    "import utils.functions as fn\n",
    "from importlib import reload\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "print(config.Config.TXT_SAVE_PATH)\n",
    "print(config.Config.MODEL_SAVE_PATH)\n",
    "\n",
    "dataset = load_from_disk(f\"../{config.Config.DATASETS_SAVE_PATH}/datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the DomainTaskAdapter class\n",
    "\n",
    "\n",
    "\n",
    "class DomainTaskAdapter(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(DomainTaskAdapter, self).__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.config = AutoConfig.from_pretrained(self.hparams[\"pretrained_model_name\"])\n",
    "        self.config.output_hidden_states = True\n",
    "        self.model = AutoAdapterModel.from_pretrained(self.hparams[\"pretrained_model_name\"], config=self.config)\n",
    "              # Define the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.hparams[\"pretrained_model_name\"])\n",
    "        self.n = self.hparams[\"source_len\"]  # Number of labeled source samples\n",
    "        self.m = self.hparams[\"target_len\"]  # Number of unlabeled target samples\n",
    "        self.lambda_factor = self.n / (self.n + self.m)\n",
    "        \n",
    "        self.reduction_factor = self.hparams.get(\"reduction_factor\", 16)\n",
    "        if self.reduction_factor == \"None\":\n",
    "            self.reduction_factor = 16\n",
    "        self.leave_out = self.hparams.get(\"leave_out\", [])\n",
    "       \n",
    "        self.saved_adapter_dir = self.hparams[\"saved_adapter_dir\"]\n",
    "        self.domain_adapter_name = self.hparams[\"domain_adapter_name\"]\n",
    "        self.task_adapter_name = self.hparams[\"task_classification_name\"]\n",
    "        \n",
    "        adapter_config = AdapterConfig.load(\"lora\", r=8, alpha=16)\n",
    "        #self.model.add_adapter(self.task_adapter_name, config=adapter_config)\n",
    "        self.model.load_adapter(f\"{self.saved_adapter_dir}/{self.domain_adapter_name}\")\n",
    "        self.model.add_classification_head(self.task_adapter_name, num_labels=self.hparams[\"num_classes\"])\n",
    "\n",
    "        #self.model.add_classification_head(self.task_adapter_name, num_labels=self.hparams[\"num_classes\"])\n",
    "        self.model.active_adapters = self.domain_adapter_name\n",
    "\n",
    "        self.model.train_adapter(self.domain_adapter_name)\n",
    "        print(self.model.adapter_summary())\n",
    "        print(fn.print_trainable_parameters(self.model))\n",
    "        print(self.model.active_head)\n",
    "\n",
    "\n",
    "        self.training_outputs = []\n",
    "        self.validation_outputs = []\n",
    "        self.test_outputs = []\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task='multiclass',                                           \n",
    "                                     num_classes=self.hparams[\"num_classes\"])\n",
    "        self.f1 = torchmetrics.F1Score(task='multiclass',num_classes=self.hparams[\"num_classes\"], average=\"macro\")\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "    def compute_mlm_loss(self, input_ids, attention_mask):\n",
    "        labels = input_ids.clone()\n",
    "        probability_matrix = torch.full(labels.shape, 0.15)\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "        input_ids[masked_indices] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        self.model.active_head = self.domain_adapter_name\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs.loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask = batch[\"source_input_ids\"], batch[\"source_attention_mask\"]\n",
    "        labels = batch[\"label_source\"]\n",
    "        self.model.active_head = self.task_adapter_name\n",
    "\n",
    "        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        accuracy = self.accuracy(labels, torch.argmax(self.softmax(logits), dim=1))\n",
    "        f1 = self.f1(labels, torch.argmax(self.softmax(logits), dim=1))\n",
    "                # Target domain data for MLM\n",
    "        target_input_ids, target_attention_mask = batch[\"target_input_ids\"], batch[\"target_attention_mask\"]\n",
    "                # Forward pass for MLM task\n",
    "        mlm_loss = self.compute_mlm_loss(target_input_ids, target_attention_mask)\n",
    "                # Mixed loss\n",
    "        mixed_loss = self.lambda_factor * loss + (1 - self.lambda_factor) * mlm_loss\n",
    "\n",
    "        self.log(\"train_classification_loss\", loss)\n",
    "        self.log(\"train_mlm_loss\", mlm_loss)\n",
    "        self.log(\"train_mixed_loss\", mixed_loss)\n",
    "\n",
    "        self.log(\"train_accuracy\", accuracy)\n",
    "        self.log(\"train_f1\", f1)\n",
    "        return mixed_loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"validation step of DomainTaskAdapter\"\"\"\n",
    "        # get the input ids and attention mask for source data\n",
    "        input_ids, attention_mask = batch[\"source_input_ids\"], batch[\"source_attention_mask\"]\n",
    "        self.model.active_head = self.task_adapter_name\n",
    "\n",
    "        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        labels = batch[\"label_source\"]\n",
    "        source_loss = self.criterion(logits, labels)\n",
    "        source_accuracy = self.accuracy(labels, torch.argmax(self.softmax(logits), dim=1))\n",
    "        source_f1 = self.f1(labels, torch.argmax(self.softmax(logits), dim=1))\n",
    "\n",
    "        # get the input ids and attention mask for target data\n",
    "        input_ids, attention_mask = batch[\"target_input_ids\"], batch[\"target_attention_mask\"]\n",
    "\n",
    "        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        labels = batch[\"label_target\"]\n",
    "        target_loss = self.criterion(logits, labels)\n",
    "        target_accuracy = self.accuracy(labels, torch.argmax(self.softmax(logits), dim=1))\n",
    "        target_f1 = self.f1(labels, torch.argmax(self.softmax(logits), dim=1))\n",
    "        # Target domain data for MLM\n",
    "        mlm_loss = self.compute_mlm_loss(input_ids, attention_mask)\n",
    "             # Mixed validation loss\n",
    "        mixed_val_loss = self.lambda_factor * source_loss + (1 - self.lambda_factor) * mlm_loss\n",
    "        # this will log the mean div value across epoch\n",
    "        self.log(\"target_val/mlm_loss\", mlm_loss)\n",
    "        self.log(\"source_val/mixed_val_loss\", mixed_val_loss)\n",
    "        self.log(name=\"source_val/loss\", value=source_loss, prog_bar=True, logger=True)\n",
    "        self.log(name=\"source_val/accuracy\", value=source_accuracy, prog_bar=True, logger=True)\n",
    "        self.log(name=\"target_val/loss\", value=target_loss, prog_bar=True, logger=True)\n",
    "        self.log(name=\"target_val/accuracy\", value=target_accuracy, prog_bar=True, logger=True)\n",
    "        self.log(name=\"target_val/f1\", value=target_f1, prog_bar=True, logger=True)\n",
    "        self.log(name=\"source_val/f1\", value=source_f1, prog_bar=True, logger=True)\n",
    "        self.validation_outputs.append({\n",
    "            \"source_val/loss\": source_loss,\n",
    "            \"source_val/accuracy\": source_accuracy,\n",
    "            \"source_val/f1\": source_f1,\n",
    "            \"target_val/loss\": target_loss,\n",
    "            \"target_val/accuracy\": target_accuracy,\n",
    "            \"target_val/f1\": target_f1,\n",
    "            \"target_val/mlm_loss\": mlm_loss,\n",
    "            \"source_val/mixed_val_loss\": mixed_val_loss,\n",
    "            \n",
    "            })\n",
    "        return {\n",
    "            \"source_val/loss\": source_loss,\n",
    "            \"source_val/accuracy\": source_accuracy,\n",
    "            \"source_val/f1\": source_f1,\n",
    "            \"target_val/loss\": target_loss,\n",
    "            \"target_val/accuracy\": target_accuracy,\n",
    "            \"target_val/f1\": target_f1,\n",
    "             \"target_val/mlm_loss\": mlm_loss,\n",
    "            \"source_val/mixed_val_loss\": mixed_val_loss,\n",
    "        }    \n",
    "    def on_validation_epoch_start(self):\n",
    "        self.validation_outputs = []\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs= self.validation_outputs\n",
    "        mean_source_loss = torch.stack([x[\"source_val/loss\"] for x in outputs]).mean()\n",
    "        mean_source_accuracy = torch.stack([x[\"source_val/accuracy\"] for x in outputs]).mean()\n",
    "        mean_source_f1 = torch.stack([x[\"source_val/f1\"] for x in outputs]).mean()\n",
    "\n",
    "        mean_target_loss = torch.stack([x[\"target_val/loss\"] for x in outputs]).mean()\n",
    "        mean_target_accuracy = torch.stack([x[\"target_val/accuracy\"] for x in outputs]).mean()\n",
    "        mean_target_f1 = torch.stack([x[\"target_val/f1\"] for x in outputs]).mean()\n",
    "        mean_mixed_loss= torch.stack([x[\"source_val/mixed_val_loss\"] for x in outputs]).mean()\n",
    "        print(f\"target_val/loss: {mean_target_loss}\")\n",
    "        print(f\"target_val/accuracy: {mean_target_accuracy}\")\n",
    "        print(f\"target_val/f1: {mean_target_accuracy}\")\n",
    "        print(f\"source_val/loss: {mean_target_loss}\")\n",
    "        print(f\"source_val/accuracy: {mean_target_accuracy}\")\n",
    "        print(f\"source_val/f1: {mean_target_accuracy}\")\n",
    "        print(f\"mean_mixed_loss: {mean_mixed_loss}\")\n",
    "        # this will log the mean div value across epoch\n",
    "        self.log(name=\"source_val/loss\", value=mean_source_loss, prog_bar=True, logger=True)\n",
    "        self.log(name=\"source_val/accuracy\", value=mean_source_accuracy, prog_bar=True, logger=True)\n",
    "\n",
    "        self.log(name=\"source_val/f1\", value=mean_source_f1, prog_bar=True, logger=True)\n",
    "        self.log(name=\"source_val/mixed_val_loss\", value=mean_mixed_loss, prog_bar=True, logger=True)\n",
    "\n",
    "        self.log(name=\"target_val/loss\", value=mean_target_loss, prog_bar=True, logger=True)\n",
    "        self.log(name=\"target_val/accuracy\", value=mean_target_accuracy, prog_bar=True, logger=True)\n",
    "        self.log(name=\"target_val/f1\", value=mean_target_f1, prog_bar=True, logger=True)\n",
    "                     # Log `val_loss` as `mean_source_loss`\n",
    "        self.log(\"val_loss\", mean_mixed_loss)\n",
    "\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"validation step of DomainTaskAdapter\"\"\"\n",
    "        # get the input ids and attention mask for source data\n",
    "        input_ids, attention_mask = batch[\"source_input_ids\"], batch[\"source_attention_mask\"]\n",
    "        self.model.active_head = self.task_adapter_name\n",
    "\n",
    "        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        labels = batch[\"label_source\"]\n",
    "        source_loss = self.criterion(logits, labels)\n",
    "        source_accuracy = self.accuracy(labels, torch.argmax(self.softmax(logits), dim=1))\n",
    "        source_f1 = self.f1(labels, torch.argmax(self.softmax(logits), dim=1))\n",
    "\n",
    "        # get the input ids and attention mask for target data\n",
    "        input_ids, attention_mask = batch[\"target_input_ids\"], batch[\"target_attention_mask\"]\n",
    "        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        labels = batch[\"label_target\"]\n",
    "        target_loss = self.criterion(logits, labels)\n",
    "        target_accuracy = self.accuracy(labels, torch.argmax(self.softmax(logits), dim=1))\n",
    "        target_f1 = self.f1(labels, torch.argmax(self.softmax(logits), dim=1))\n",
    "\n",
    "        # this will log the mean div value across epoch\n",
    "        self.log(name=\"source_test/loss\", value=source_loss)\n",
    "        self.log(name=\"source_test/accuracy\", value=source_accuracy)\n",
    "        self.log(name=\"target_test/loss\", value=target_loss)\n",
    "        self.log(name=\"target_test/accuracy\", value=target_accuracy)\n",
    "        self.log(name=\"target_test/f1\", value=target_f1)\n",
    "        self.log(name=\"source_test/f1\", value=source_f1)\n",
    "        self.test_outputs.append({\n",
    "            \"source_test/loss\": source_loss,\n",
    "            \"source_test/accuracy\": source_accuracy,\n",
    "            \"source_test/f1\": source_f1,\n",
    "            \"target_test/loss\": target_loss,\n",
    "            \"target_test/accuracy\": target_accuracy,\n",
    "            \"target_test/f1\": target_f1,\n",
    "         })\n",
    "        # need not to log here (or we can do it but let's log at the end of each epoch)\n",
    "        return {\n",
    "            \"source_test/loss\": source_loss,\n",
    "            \"source_test/accuracy\": source_accuracy,\n",
    "            \"source_test/f1\": source_f1,\n",
    "            \"target_test/loss\": target_loss,\n",
    "            \"target_test/accuracy\": target_accuracy,\n",
    "            \"target_test/f1\": target_f1,\n",
    "        }\n",
    "    def on_test_epoch_start(self):\n",
    "        self.test_outputs = []\n",
    "    def on_test_epoch_end(self):\n",
    "        outputs=  self.test_outputs\n",
    "        mean_source_loss = torch.stack([x[\"source_test/loss\"] for x in outputs]).mean()\n",
    "        mean_source_accuracy = torch.stack([x[\"source_test/accuracy\"] for x in outputs]).mean()\n",
    "        mean_source_f1 = torch.stack([x[\"source_test/f1\"] for x in outputs]).mean()\n",
    "\n",
    "        mean_target_loss = torch.stack([x[\"target_test/loss\"] for x in outputs]).mean()\n",
    "        mean_target_accuracy = torch.stack([x[\"target_test/accuracy\"] for x in outputs]).mean()\n",
    "        mean_target_f1 = torch.stack([x[\"target_test/f1\"] for x in outputs]).mean()\n",
    "\n",
    "        # this will log the mean div value across epoch\n",
    "        self.log(name=\"source_test/loss\", value=mean_source_loss)\n",
    "        self.log(name=\"source_test/accuracy\", value=mean_source_accuracy)\n",
    "        self.log(name=\"target_test/loss\", value=mean_target_loss)\n",
    "        self.log(name=\"target_test/accuracy\", value=mean_target_accuracy)\n",
    "        self.log(name=\"target_test/f1\", value=mean_target_f1)\n",
    "        self.log(name=\"source_test/f1\", value=mean_source_f1)\n",
    "    def save_adapter(self, location, adapter_name):\n",
    "        self.model.save_adapter(location, adapter_name)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams[\"learning_rate\"])\n",
    "        lr_scheduler = {\n",
    "            'scheduler': optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, threshold=0.0001, cooldown=0, min_lr=1e-8),\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Training and Evaluation Loop with Wandb logging\n",
    "import wandb\n",
    "wandb.login()\n",
    "# Wandb setup and training loop\n",
    "seeds = [42]  # List of seeds\n",
    "project_name = 'mixed'  # Replace with your wandb project name\n",
    "domain = 'aug_telephone_government'  # Replace with the specific domain for this notebook\n",
    "type = 'inv_aug'  # Replace with the specific type for this notebook\n",
    "domain_aprev ='TEG_aug'\n",
    "results = {\n",
    "    \"source_val/loss\": [],\n",
    "    \"source_val/accuracy\": [],\n",
    "    \"source_val/f1\": [],\n",
    "    \"target_val/loss\": [],\n",
    "    \"target_val/accuracy\": [],\n",
    "    \"target_val/f1\": [],\n",
    "    \"source_test/loss\": [],\n",
    "    \"source_test/accuracy\": [],\n",
    "    \"source_test/f1\": [],\n",
    "    \"target_test/loss\": [],\n",
    "    \"target_test/accuracy\": [],\n",
    "    \"target_test/f1\": [],\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "best_model_path = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/guest/Desktop/projects/third-experiments/domain_adaptation_project/mixed/government/wandb/run-20240612_173823-vmqbsemj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mrawhani/mixed/runs/vmqbsemj' target=\"_blank\">aug_telephone_government_inv_aug_run_with_seed_42</a></strong> to <a href='https://wandb.ai/mrawhani/mixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mrawhani/mixed' target=\"_blank\">https://wandb.ai/mrawhani/mixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mrawhani/mixed/runs/vmqbsemj' target=\"_blank\">https://wandb.ai/mrawhani/mixed/runs/vmqbsemj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prinssst: telephone\n",
      "print: government\n",
      "print: 69615\n",
      "prinssst: telephone\n",
      "print: government\n",
      "print: 69615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "Missing logger folder: /home/guest/Desktop/projects/third-experiments/domain_adaptation_project/mixed/government/lightning_logs\n",
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/guest/Desktop/projects/third-experiments/domain_adaptation_project/mixed/government/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Name                     Architecture         #Param      %Param  Active   Train\n",
      "--------------------------------------------------------------------------------\n",
      "mlm_inv_G_aug            bottleneck        7,387,776       6.748       1       1\n",
      "--------------------------------------------------------------------------------\n",
      "Full model                               109,482,240     100.000               0\n",
      "================================================================================\n",
      "trainable params: 9195453 || all params: 118708215 || trainable%: 7.746265075251953\n",
      "None\n",
      "mlm_inv_G_aug\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | BertAdapterModel   | 118 M \n",
      "1 | criterion | CrossEntropyLoss   | 0     \n",
      "2 | accuracy  | MulticlassAccuracy | 0     \n",
      "3 | f1        | MulticlassF1Score  | 0     \n",
      "4 | softmax   | Softmax            | 0     \n",
      "-------------------------------------------------\n",
      "9.2 M     Trainable params\n",
      "109 M     Non-trainable params\n",
      "118 M     Total params\n",
      "474.833   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a67b07776441ed80980cc881ec856b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_val/loss: 1.0942919254302979\n",
      "target_val/accuracy: 0.328125\n",
      "target_val/f1: 0.328125\n",
      "source_val/loss: 1.0942919254302979\n",
      "source_val/accuracy: 0.328125\n",
      "source_val/f1: 0.328125\n",
      "mean_mixed_loss: 1.4418991804122925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072167e4356a41a7b90bc5a2813e7912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1e483d539940c58f56e75e04df3123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_val/loss: 0.642625629901886\n",
      "target_val/accuracy: 0.7448908090591431\n",
      "target_val/f1: 0.7448908090591431\n",
      "source_val/loss: 0.642625629901886\n",
      "source_val/accuracy: 0.7448908090591431\n",
      "source_val/f1: 0.7448908090591431\n",
      "mean_mixed_loss: 1.0473644733428955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8a2bff4cc24a23844ccfca3694c914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_val/loss: 0.6155364513397217\n",
      "target_val/accuracy: 0.7549630999565125\n",
      "target_val/f1: 0.7549630999565125\n",
      "source_val/loss: 0.6155364513397217\n",
      "source_val/accuracy: 0.7549630999565125\n",
      "source_val/f1: 0.7549630999565125\n",
      "mean_mixed_loss: 1.0262619256973267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54cfb019e364e31b749adbed46343a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_val/loss: 0.5784991383552551\n",
      "target_val/accuracy: 0.7755737900733948\n",
      "target_val/f1: 0.7755737900733948\n",
      "source_val/loss: 0.5784991383552551\n",
      "source_val/accuracy: 0.7755737900733948\n",
      "source_val/f1: 0.7755737900733948\n",
      "mean_mixed_loss: 1.0139967203140259\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b52dbeec754dd4a678d86f2fb23406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_val/loss: 0.5872280597686768\n",
      "target_val/accuracy: 0.778824508190155\n",
      "target_val/f1: 0.778824508190155\n",
      "source_val/loss: 0.5872280597686768\n",
      "source_val/accuracy: 0.778824508190155\n",
      "source_val/f1: 0.778824508190155\n",
      "mean_mixed_loss: 1.0118035078048706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f737adda71d34298b1c228768322550d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_val/loss: 0.6346541047096252\n",
      "target_val/accuracy: 0.7860334515571594\n",
      "target_val/f1: 0.7860334515571594\n",
      "source_val/loss: 0.6346541047096252\n",
      "source_val/accuracy: 0.7860334515571594\n",
      "source_val/f1: 0.7860334515571594\n",
      "mean_mixed_loss: 1.0547696352005005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4116d4e4b9948779f5bdba90539cf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_val/loss: 0.6887211799621582\n",
      "target_val/accuracy: 0.7847421765327454\n",
      "target_val/f1: 0.7847421765327454\n",
      "source_val/loss: 0.6887211799621582\n",
      "source_val/accuracy: 0.7847421765327454\n",
      "source_val/f1: 0.7847421765327454\n",
      "mean_mixed_loss: 1.0804874897003174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4ad02a188443e384baeb0b10811f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_val/loss: 0.6477600336074829\n",
      "target_val/accuracy: 0.7864995002746582\n",
      "target_val/f1: 0.7864995002746582\n",
      "source_val/loss: 0.6477600336074829\n",
      "source_val/accuracy: 0.7864995002746582\n",
      "source_val/f1: 0.7864995002746582\n",
      "mean_mixed_loss: 1.0569517612457275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Name                     Architecture         #Param      %Param  Active   Train\n",
      "--------------------------------------------------------------------------------\n",
      "mlm_inv_G_aug            bottleneck        7,387,776       6.748       1       1\n",
      "--------------------------------------------------------------------------------\n",
      "Full model                               109,482,240     100.000               0\n",
      "================================================================================\n",
      "trainable params: 9195453 || all params: 118708215 || trainable%: 7.746265075251953\n",
      "None\n",
      "mlm_inv_G_aug\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prinssst: telephone\n",
      "print: government\n",
      "print: 69615\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b09fdc1aa742a58ef033d8fc805db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   source_test/accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.775860607624054     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      source_test/f1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7675790190696716     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     source_test/loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5938614010810852     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   target_test/accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7687499523162842     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      target_test/f1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7630484700202942     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     target_test/loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5973318815231323     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m  source_test/accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.775860607624054    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     source_test/f1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7675790190696716    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    source_test/loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5938614010810852    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  target_test/accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7687499523162842    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     target_test/f1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7630484700202942    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    target_test/loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5973318815231323    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: [{'source_test/loss': 0.5938614010810852, 'source_test/accuracy': 0.775860607624054, 'target_test/loss': 0.5973318815231323, 'target_test/accuracy': 0.7687499523162842, 'target_test/f1': 0.7630484700202942, 'source_test/f1': 0.7675790190696716}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae71d156fd224a7ba9cd5cae3190fc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">aug_telephone_government_inv_aug_run_with_seed_42</strong> at: <a href='https://wandb.ai/mrawhani/mixed/runs/vmqbsemj' target=\"_blank\">https://wandb.ai/mrawhani/mixed/runs/vmqbsemj</a><br/> View project at: <a href='https://wandb.ai/mrawhani/mixed' target=\"_blank\">https://wandb.ai/mrawhani/mixed</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240612_173823-vmqbsemj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(processed)\n",
    "for seed in seeds:\n",
    "    wandb.init(project=project_name, name=f'{domain}_{type}_run_with_seed_{seed}', config={'seed': seed})\n",
    "\n",
    "    try:\n",
    "    \n",
    "\n",
    "        seed_everything(seed)\n",
    "\n",
    "        # Hyperparameters\n",
    "        hparams = {\n",
    "            \"source_target\": \"aug_telephone_government\",\n",
    "            \"source_domain\": \"telephone\",\n",
    "            \"target_domain\": \"government\",\n",
    "            \"domain_adapter_name\": \"mlm_inv_G_aug\",\n",
    "            \"task_classification_name\": \"mixed_aug_TEG_task_inv\",\n",
    "            \"pretrained_model_name\": \"bert-base-uncased\",\n",
    "            \"padding\": \"max_length\",\n",
    "            \"max_seq_length\": 128,\n",
    "            \"bsz\": 32,\n",
    "            \"num_classes\": 3,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"reduction_factor\": 16,\n",
    "           \n",
    "            \"mode\": \"domain\",\n",
    "            \"saved_adapter_dir\": \"../../saved/adapters\",\n",
    "        }\n",
    "        dm = processed.DataModuleSourceTarget(hparams)\n",
    "        dm.setup('fit')\n",
    "        dm.setup(\"test\")\n",
    "\n",
    "        \n",
    "\n",
    "        train_loader = dm.train_dataloader()\n",
    "        val_loader = dm.val_dataloader()\n",
    "\n",
    "\n",
    "        hparams['source_len'] = len(train_loader.dataset.source_df)\n",
    "        hparams['target_len'] = len(train_loader.dataset.target_df)\n",
    "\n",
    "\n",
    "        model = DomainTaskAdapter(hparams)\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            filename=\"mixed_aug_TEG_task_inv-{epoch:02d}-{val_loss:.2f}\",\n",
    "            save_top_k=1,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=3,\n",
    "            mode=\"min\",\n",
    "        )\n",
    "        wandb_logger = WandbLogger()\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing : {e}\")   \n",
    "\n",
    "    try:\n",
    "      \n",
    "        trainer = Trainer(\n",
    "            max_epochs=10,\n",
    "            accelerator=\"auto\",\n",
    "            #logger=wandb_logger,\n",
    "            callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "            limit_train_batches=1.0,\n",
    "            limit_val_batches=1.0,\n",
    "            limit_test_batches=1.0,\n",
    "            log_every_n_steps=5,\n",
    "        )\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training : {e}\")\n",
    "\n",
    "    try:\n",
    "        best_ckpt_path = checkpoint_callback.best_model_path\n",
    "        model = DomainTaskAdapter.load_from_checkpoint(best_ckpt_path)\n",
    "        dm.setup(\"test\")\n",
    "        test_loader = dm.test_dataloader()\n",
    "        test_results = trainer.test(model, test_loader)\n",
    "        print(\"Test Results:\", test_results)\n",
    "\n",
    "        # # Collect the results for averaging later\n",
    "        # for key in results.keys():\n",
    "        #     results[key].append(wandb.run.summary[key])\n",
    "\n",
    "        # dm.setup(\"fit\")\n",
    "        # val_loader = dm.val_dataloader()\n",
    "\n",
    "        # # Get the metrics for the best epoch\n",
    "        # best_metrics = trainer.validate(model, dataloaders=val_loader, verbose=False)[0]\n",
    "        #   # Log the best metrics to wandb summary\n",
    "        # for key, value in best_metrics.items():\n",
    "        #     wandb.run.summary[key] = value\n",
    "\n",
    "      \n",
    "        # # Append validation metrics to results dictionary\n",
    "        # for key in best_metrics.keys(): \n",
    "        #     if key not in results:\n",
    "        #         results[key] = []\n",
    "        #     results[key].append(best_metrics[key])\n",
    "\n",
    "        # # Save the best model based on validation loss\n",
    "        # current_val_loss = wandb.run.summary[\"source_val/loss\"]\n",
    "        # if current_val_loss < best_val_loss:\n",
    "        #     best_val_loss = current_val_loss\n",
    "        #     best_model = model\n",
    "        #     best_model_path = best_ckpt_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {e}\")\n",
    "\n",
    "    # Finish the wandb run\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('source_val/loss', []), ('source_val/accuracy', []), ('source_val/f1', []), ('target_val/loss', []), ('target_val/accuracy', []), ('target_val/f1', []), ('source_test/loss', []), ('source_test/accuracy', []), ('source_test/f1', []), ('target_test/loss', []), ('target_test/accuracy', []), ('target_test/f1', [])])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate mean results\n",
    "mean_results = {key: sum(values) / len(values) for key, values in results.items()}\n",
    "\n",
    "# # Log mean results to wandb\n",
    "# wandb.init(project=project_name, name=f'{domain}_mean_results')\n",
    "# for key, value in mean_results.items():\n",
    "#     wandb.log({key: value})\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"Mean Results:\", mean_results)\n",
    "\n",
    "# Save the best model's adapter\n",
    "if model:\n",
    "    adapter_save_path = f\"../../saved/adapter_after_run/{hparams['task_classification_name']}\"\n",
    "    model.save_adapter(adapter_save_path, hparams['domain_adapter_name'])\n",
    "    print(f\"Adapter saved to {adapter_save_path}\")\n",
    "else:\n",
    "    print(\"No best model to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dones\n"
     ]
    }
   ],
   "source": [
    "print('dones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6173854470252991"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
