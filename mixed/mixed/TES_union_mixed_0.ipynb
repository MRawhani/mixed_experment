{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4101efcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T13:12:17.986836Z",
     "iopub.status.busy": "2024-08-30T13:12:17.985917Z",
     "iopub.status.idle": "2024-08-30T13:12:18.749326Z",
     "shell.execute_reply": "2024-08-30T13:12:18.748948Z"
    },
    "papermill": {
     "duration": 0.77373,
     "end_time": "2024-08-30T13:12:18.750722",
     "exception": false,
     "start_time": "2024-08-30T13:12:17.976992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2dab711",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T13:12:18.755827Z",
     "iopub.status.busy": "2024-08-30T13:12:18.755526Z",
     "iopub.status.idle": "2024-08-30T13:12:22.257405Z",
     "shell.execute_reply": "2024-08-30T13:12:22.256995Z"
    },
    "papermill": {
     "duration": 3.505571,
     "end_time": "2024-08-30T13:12:22.258472",
     "exception": false,
     "start_time": "2024-08-30T13:12:18.752901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages', '/tmp/tmp_5uryu58', '/home/guest/Desktop/projects/third-experiments/domain_adaptation_project/modules']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 16:12:20.822967: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-30 16:12:20.855029: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 16:12:21.515283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./text-files/\n",
      "./hp-model-\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Initialize the console\n",
    "\n",
    "# Step 2: Import necessary libraries\n",
    "from typing import Optional, Dict, Any\n",
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rich.traceback import install\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoConfig\n",
    "from adapters import AutoAdapterModel, AdapterConfig\n",
    "from adapters.composition import Stack\n",
    "from pytorch_lightning import seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint,EarlyStopping\n",
    "import torchmetrics\n",
    "\n",
    "install(show_locals=True)\n",
    "\n",
    "from setup import setup_src_path\n",
    "print(setup_src_path())\n",
    "import data.processed as processed\n",
    "import config.config as config\n",
    "import utils.setup as setup\n",
    "import utils.functions as fn\n",
    "from importlib import reload\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "print(config.Config.TXT_SAVE_PATH)\n",
    "print(config.Config.MODEL_SAVE_PATH)\n",
    "\n",
    "dataset = load_from_disk(f\"../{config.Config.DATASETS_SAVE_PATH}/datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674990e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T13:12:22.269829Z",
     "iopub.status.busy": "2024-08-30T13:12:22.269357Z",
     "iopub.status.idle": "2024-08-30T13:12:22.291983Z",
     "shell.execute_reply": "2024-08-30T13:12:22.291580Z"
    },
    "papermill": {
     "duration": 0.033103,
     "end_time": "2024-08-30T13:12:22.293096",
     "exception": false,
     "start_time": "2024-08-30T13:12:22.259993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from transformers import  AutoConfig, DataCollatorForLanguageModeling\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "\n",
    "class JointDomainTaskAdapter(pl.LightningModule):\n",
    "    def __init__(self, hparams,source_dataset_length,target_dataset_length):\n",
    "        super(JointDomainTaskAdapter, self).__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "        # Load config with hidden states output\n",
    "        self.config = AutoConfig.from_pretrained(self.hparams[\"pretrained_model_name\"])\n",
    "        self.config.output_hidden_states = True\n",
    "        self.model = AutoAdapterModel.from_pretrained(self.hparams[\"pretrained_model_name\"], config=self.config)\n",
    "\n",
    "        # Set reduction factor and leave_out layers\n",
    "        self.reduction_factor = self.hparams.get(\"reduction_factor\", 16)\n",
    "        self.leave_out = self.hparams.get(\"leave_out\", [])\n",
    "        # if self.leave_out != \"None\":\n",
    "        #     self.leave_out = self.leave_out.split(\",\")\n",
    "        #     self.leave_out = [int(i) for i in self.leave_out]\n",
    "        # else:\n",
    "        #     self.leave_out = []\n",
    "\n",
    "        # Load MLM adapter with head\n",
    "        self.model.load_adapter(f\"{self.hparams['saved_adapter_dir']}/{self.hparams['domain_adapter_name']}\", with_head=True)\n",
    "\n",
    "        # Add classification head for the task\n",
    "        self.model.add_classification_head(f\"{self.hparams['task_adapter_name']}\", num_labels=self.hparams[\"num_classes\"])\n",
    "\n",
    "        # Set active adapters\n",
    "        self.model.train_adapter(self.hparams['domain_adapter_name'])\n",
    "        # Calculate alpha based on dataset lengths\n",
    "        self.alpha = source_dataset_length / (source_dataset_length + target_dataset_length)\n",
    "\n",
    "        # Initialize loss functions and metrics\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.mlm_criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=self.hparams[\"num_classes\"])\n",
    "        self.f1 = torchmetrics.F1Score(task='multiclass', num_classes=self.hparams[\"num_classes\"], average=\"weighted\")\n",
    "        self.f1_macro = torchmetrics.F1Score(task='multiclass', num_classes=self.hparams[\"num_classes\"], average=\"macro\")\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.validation_outputs = []\n",
    "        self.test_outputs = []\n",
    "        # Optimizer related variables\n",
    "        self.learning_rate = self.hparams.get(\"learning_rate\", 1e-4)\n",
    "        self.scheduler_factor = self.hparams.get(\"scheduler_factor\", 0.1)\n",
    "        self.scheduler_patience = self.hparams.get(\"scheduler_patience\", 0.05)\n",
    "        self.scheduler_threshold = self.hparams.get(\"scheduler_threshold\", 0.0001)\n",
    "        self.scheduler_cooldown = self.hparams.get(\"scheduler_cooldown\", 0)\n",
    "        self.scheduler_eps = self.hparams.get(\"scheduler_eps\", 1e-8)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, task=None):\n",
    "        if task == \"mlm\":\n",
    "            self.model.active_head= self.hparams['domain_adapter_name']\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        elif task == \"classification\":\n",
    "            self.model.active_head= self.hparams['task_adapter_name']\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        else:\n",
    "            raise ValueError(\"Task must be either 'mlm' or 'classification'.\")\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Separate source and target data processing\n",
    "        source_input_ids = batch[\"source_input_ids\"]\n",
    "        source_attention_mask = batch[\"source_attention_mask\"]\n",
    "        source_labels = batch[\"label_source\"]\n",
    "\n",
    "        target_input_ids = batch[\"target_input_ids\"]\n",
    "        target_attention_mask = batch[\"target_attention_mask\"]\n",
    "        mlm_labels = batch[\"mlm_labels\"]\n",
    "\n",
    "        # Calculate dynamic alpha based on the lengths of source and target data\n",
    "        alpha = self.alpha\n",
    "\n",
    "        # Classification task\n",
    "        cls_outputs = self(input_ids=source_input_ids, attention_mask=source_attention_mask, task=\"classification\")\n",
    "        cls_logits = cls_outputs.logits\n",
    "        task_loss = self.criterion(cls_logits, source_labels)\n",
    "\n",
    "        # MLM task\n",
    "        mlm_outputs = self(input_ids=target_input_ids, attention_mask=target_attention_mask, labels=mlm_labels, task=\"mlm\")\n",
    "        mlm_loss = mlm_outputs.loss\n",
    "\n",
    "        # Combine losses\n",
    "        loss = alpha * task_loss + (1 - alpha) * mlm_loss\n",
    "\n",
    "        accuracy = self.accuracy(source_labels, torch.argmax(self.softmax(cls_logits), dim=1))\n",
    "        f1 = self.f1(source_labels, torch.argmax(self.softmax(cls_logits), dim=1))\n",
    "\n",
    "        metrics = {\n",
    "            \"train/accuracy\": accuracy,\n",
    "            \"train/f1\": f1,\n",
    "            \"train/taskclf_loss\": task_loss,\n",
    "            \"train/loss\": loss,\n",
    "            \"train/mlm_loss\": mlm_loss\n",
    "        }\n",
    "\n",
    "        for key, val in metrics.items():\n",
    "            self.log(name=key, value=val)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Separate source and target data processing\n",
    "        source_input_ids = batch[\"source_input_ids\"]\n",
    "        source_attention_mask = batch[\"source_attention_mask\"]\n",
    "        source_labels = batch[\"label_source\"]\n",
    "\n",
    "        target_input_ids = batch[\"target_input_ids\"]\n",
    "        target_attention_mask = batch[\"target_attention_mask\"]\n",
    "        mlm_labels = batch[\"mlm_labels\"]\n",
    "\n",
    "        # Calculate dynamic alpha based on the lengths of source and target data\n",
    "        alpha = self.alpha\n",
    "        # Classification task\n",
    "        cls_outputs = self(input_ids=source_input_ids, attention_mask=source_attention_mask, task=\"classification\")\n",
    "        cls_logits = cls_outputs.logits\n",
    "        task_loss = self.criterion(cls_logits, source_labels)\n",
    "\n",
    "        # MLM task\n",
    "        mlm_outputs = self(input_ids=target_input_ids, attention_mask=target_attention_mask, labels=mlm_labels, task=\"mlm\")\n",
    "        mlm_loss = mlm_outputs.loss\n",
    "\n",
    "        # Combine losses\n",
    "        loss = alpha * task_loss + (1 - alpha) * mlm_loss\n",
    "\n",
    "        accuracy = self.accuracy(source_labels, torch.argmax(self.softmax(cls_logits), dim=1))\n",
    "        f1 = self.f1(source_labels, torch.argmax(self.softmax(cls_logits), dim=1))\n",
    "        self.validation_outputs.append({\n",
    "            \"val/accuracy\": accuracy,\n",
    "            \"val/f1\": f1,\n",
    "            \"val/taskclf_loss\": task_loss,\n",
    "            \"val/loss\": loss,\n",
    "            \"val/mlm_loss\": mlm_loss\n",
    "                })\n",
    "        \n",
    "        metrics = {\n",
    "            \"val/accuracy\": accuracy,\n",
    "            \"val/f1\": f1,\n",
    "            \"val/taskclf_loss\": task_loss,\n",
    "            \"val/loss\": loss,\n",
    "            \"val/mlm_loss\": mlm_loss\n",
    "        }\n",
    "\n",
    "        for key, val in metrics.items():\n",
    "            self.log(name=key, value=val)\n",
    "\n",
    "        return metrics\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.validation_outputs = []\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs= self.validation_outputs\n",
    "\n",
    "        avg_loss = torch.stack([x[\"val/loss\"] for x in outputs]).mean()\n",
    "        avg_task_loss = torch.stack([x[\"val/taskclf_loss\"] for x in outputs]).mean()\n",
    "        avg_mlm_loss = torch.stack([x[\"val/mlm_loss\"] for x in outputs]).mean()\n",
    "        avg_accuracy = torch.stack([x[\"val/accuracy\"] for x in outputs]).mean()\n",
    "        avg_f1 = torch.stack([x[\"val/f1\"] for x in outputs]).mean()\n",
    "        print(f\"val/accuracy: {avg_accuracy}\")\n",
    "        print(f\"val/f1: {avg_f1}\")\n",
    "        print(f\"val/taskclf_loss: {avg_task_loss}\")\n",
    "        print(f\"val/loss: {avg_loss}\")\n",
    "        print(f\"val/mlm_loss: {avg_mlm_loss}\")\n",
    "        metrics = {\n",
    "            \"val/avg_loss\": avg_loss,\n",
    "            \"val/avg_taskclf_loss\": avg_task_loss,\n",
    "            \"val/avg_mlm_loss\": avg_mlm_loss,\n",
    "            \"val/avg_accuracy\": avg_accuracy,\n",
    "            \"val/avg_f1\": avg_f1,\n",
    "        }\n",
    "\n",
    "        for key, val in metrics.items():\n",
    "            self.log(name=key, value=val)\n",
    "        self.log(\"val_loss\", avg_loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Separate source and target data processing\n",
    "        source_input_ids = batch[\"source_input_ids\"]\n",
    "        source_attention_mask = batch[\"source_attention_mask\"]\n",
    "        source_labels = batch[\"label_source\"]\n",
    "\n",
    "        target_input_ids = batch[\"target_input_ids\"]\n",
    "        target_attention_mask = batch[\"target_attention_mask\"]\n",
    "        target_labels = batch[\"label_target\"]\n",
    "\n",
    "        # Classification task for source data\n",
    "        cls_outputs_source = self(input_ids=source_input_ids, attention_mask=source_attention_mask, task=\"classification\")\n",
    "        cls_logits_source = cls_outputs_source.logits\n",
    "        task_loss_source = self.criterion(cls_logits_source, source_labels)\n",
    "\n",
    "        # Classification task for target data\n",
    "        cls_outputs_target = self(input_ids=target_input_ids, attention_mask=target_attention_mask, task=\"classification\")\n",
    "        cls_logits_target = cls_outputs_target.logits\n",
    "        task_loss_target = self.criterion(cls_logits_target, target_labels)\n",
    "\n",
    "        # Combine losses (though typically you would evaluate them separately)\n",
    "        loss = task_loss_source + task_loss_target\n",
    "\n",
    "        accuracy_source = self.accuracy(source_labels, torch.argmax(self.softmax(cls_logits_source), dim=1))\n",
    "        f1_source = self.f1(source_labels, torch.argmax(self.softmax(cls_logits_source), dim=1))\n",
    "        f1_macro_source = self.f1_macro(source_labels, torch.argmax(self.softmax(cls_logits_source), dim=1))\n",
    "\n",
    "        accuracy_target = self.accuracy(target_labels, torch.argmax(self.softmax(cls_logits_target), dim=1))\n",
    "        f1_target = self.f1(target_labels, torch.argmax(self.softmax(cls_logits_target), dim=1))\n",
    "        f1_macro_target = self.f1_macro(target_labels, torch.argmax(self.softmax(cls_logits_target), dim=1))\n",
    "\n",
    "        metrics = {\n",
    "            \"source_test/loss\": task_loss_source,\n",
    "            \"source_test/accuracy\": accuracy_source,\n",
    "            \"source_test/f1\": f1_source,\n",
    "            \"source_test/f1_macro\": f1_macro_source,\n",
    "            \"target_test/loss\": task_loss_target,\n",
    "            \"target_test/accuracy\": accuracy_target,\n",
    "            \"target_test/f1\": f1_target,\n",
    "            \"target_test/f1_macro\": f1_macro_target,\n",
    "        }\n",
    "\n",
    "        for key, val in metrics.items():\n",
    "            self.log(name=key, value=val)\n",
    "\n",
    "        self.test_outputs.append({\n",
    "            \"source_test/loss\": task_loss_source,\n",
    "            \"source_test/accuracy\": accuracy_source,\n",
    "            \"source_test/f1\": f1_source,\n",
    "            \"source_test/f1_macro\": f1_macro_source,\n",
    "            \"target_test/loss\": task_loss_target,\n",
    "            \"target_test/accuracy\": accuracy_target,\n",
    "            \"target_test/f1\": f1_target,\n",
    "            \"target_test/f1_macro\": f1_macro_target,\n",
    "        })\n",
    "        return metrics\n",
    "    def on_test_epoch_start(self):\n",
    "        self.test_outputs = []\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        outputs=  self.test_outputs\n",
    "\n",
    "        avg_loss_source = torch.stack([x[\"source_test/loss\"] for x in outputs]).mean()\n",
    "        avg_task_loss_target = torch.stack([x[\"target_test/loss\"] for x in outputs]).mean()\n",
    "        avg_accuracy_source = torch.stack([x[\"source_test/accuracy\"] for x in outputs]).mean()\n",
    "        avg_f1_source = torch.stack([x[\"source_test/f1\"] for x in outputs]).mean()\n",
    "        avg_f1_macro_source = torch.stack([x[\"source_test/f1_macro\"] for x in outputs]).mean()\n",
    "\n",
    "        avg_accuracy_target = torch.stack([x[\"target_test/accuracy\"] for x in outputs]).mean()\n",
    "        avg_f1_target = torch.stack([x[\"target_test/f1\"] for x in outputs]).mean()\n",
    "        avg_f1_macro_target = torch.stack([x[\"target_test/f1_macro\"] for x in outputs]).mean()\n",
    "\n",
    "        metrics = {\n",
    "            \"source_test/loss\": avg_loss_source,\n",
    "            \"target_test/loss\": avg_task_loss_target,\n",
    "            \"source_test/accuracy\": avg_accuracy_source,\n",
    "            \"source_test/f1\": avg_f1_source,\n",
    "            \"source_test/f1_macro\": avg_f1_macro_source,\n",
    "            \"target_test/accuracy\": avg_accuracy_target,\n",
    "            \"target_test/f1\": avg_f1_target,\n",
    "            \"target_test/f1_macro\": avg_f1_macro_target,\n",
    "        }\n",
    "\n",
    "        for key, val in metrics.items():\n",
    "            self.log(name=key, value=val)\n",
    "    def save_adapter(self, location, adapter_name):\n",
    "        self.model.save_adapter(location, adapter_name)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=self.scheduler_factor,\n",
    "            patience=self.scheduler_patience,\n",
    "            threshold=self.scheduler_threshold,\n",
    "            cooldown=self.scheduler_cooldown,\n",
    "            eps=self.scheduler_eps,\n",
    "            verbose=True,\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"reduce_lr_on_plateau\": True, \"monitor\": \"val_loss\", \"interval\": \"epoch\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e722512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T13:12:22.297863Z",
     "iopub.status.busy": "2024-08-30T13:12:22.297679Z",
     "iopub.status.idle": "2024-08-30T13:12:23.527583Z",
     "shell.execute_reply": "2024-08-30T13:12:23.525208Z"
    },
    "papermill": {
     "duration": 1.236773,
     "end_time": "2024-08-30T13:12:23.532099",
     "exception": false,
     "start_time": "2024-08-30T13:12:22.295326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrawhani5\u001b[0m (\u001b[33mmrawhani\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "# Wandb setup and training loop\n",
    "seeds = [42, 10, 100]  # List of seeds\n",
    "project_name = 'mixed_edited'  # Replace with your wandb project name\n",
    "domain = 'TES'  # Replace with the specific domain for this notebook\n",
    "type = 'union'  # Replace with the specific type for this notebook\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {\n",
    "    \"last_epoch\": {\n",
    "        \"source_test/loss\": [],\n",
    "        \"source_test/accuracy\": [],\n",
    "        \"source_test/f1\": [],\n",
    "        \"source_test/f1_macro\": [],\n",
    "        \"target_test/loss\": [],\n",
    "        \"target_test/accuracy\": [],\n",
    "        \"target_test/f1\": [],\n",
    "        \"target_test/f1_macro\": [],\n",
    "    },\n",
    "    \"best_model\": {\n",
    "        \"source_test/loss\": [],\n",
    "        \"source_test/accuracy\": [],\n",
    "        \"source_test/f1\": [],\n",
    "        \"source_test/f1_macro\": [],\n",
    "        \"target_test/loss\": [],\n",
    "        \"target_test/accuracy\": [],\n",
    "        \"target_test/f1\": [],\n",
    "        \"target_test/f1_macro\": [],\n",
    "    },\n",
    "    \"epoch_saved\": {\n",
    "        \"source_test/loss\": [],\n",
    "        \"source_test/accuracy\": [],\n",
    "        \"source_test/f1\": [],\n",
    "        \"source_test/f1_macro\": [],\n",
    "        \"target_test/loss\": [],\n",
    "        \"target_test/accuracy\": [],\n",
    "        \"target_test/f1\": [],\n",
    "        \"target_test/f1_macro\": [],\n",
    "    }\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "best_model_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35aa283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T13:12:23.547487Z",
     "iopub.status.busy": "2024-08-30T13:12:23.546847Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-08-30T13:12:23.535105",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/guest/Desktop/projects/third-experiments/domain_adaptation_project/mixed/mixed/wandb/run-20240830_161223-kpvoid2e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTES_union_run_with_seed_42\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrawhani/mixed_edited\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrawhani/mixed_edited/runs/kpvoid2e\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cd0288575e47d8bcb17041ffe2a74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8f9ef8ddcd4cab923ae4e7dda469aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49eb3feefc74c83955e1a2932728d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a768707852a747798374f12cc130004d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source genre: telephone\n",
      "Target genre: slate\n",
      "Number of target samples: 69575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f1bfd398a843a9ac75a5fc7a729d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf815e1492b44fb586352788ab17dd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023cca05ee9040d8ac2142fa6d5b789e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f63c71e2b6e4118a7eaefb799c0f6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b0ba0d5a454598b8f768da00b64f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddf6a69a1d8483b831f9233da1dae9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922c9c805a5e42eaa1ced34eb6ed2bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0050424ba2f4753ac55e8e3a5a19353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source genre: telephone\n",
      "Target genre: slate\n",
      "Number of target samples: 69575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc1216fffd54748a460edda9dedb2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31f7e826ac6495c809399b012e28b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6a595a49a8445c8b450b339e538277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d2e51746114561861ff739c7c75027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source dataset length: 75013\n",
      "Target dataset length: 21585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/lightning_fabric/connector.py:563: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:257: Found unsupported keys in the lr scheduler dict: {'reduce_lr_on_plateau'}. HINT: remove them from the output of `configure_optimizers`.\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | model         | BertAdapterModel   | 119 M \n",
      "1 | criterion     | CrossEntropyLoss   | 0     \n",
      "2 | mlm_criterion | CrossEntropyLoss   | 0     \n",
      "3 | accuracy      | MulticlassAccuracy | 0     \n",
      "4 | f1            | MulticlassF1Score  | 0     \n",
      "5 | f1_macro      | MulticlassF1Score  | 0     \n",
      "6 | softmax       | Softmax            | 0     \n",
      "-----------------------------------------------------\n",
      "9.5 M     Trainable params\n",
      "109 M     Non-trainable params\n",
      "119 M     Total params\n",
      "476.013   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71c967ea58c4e38a658f4bc0db301bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.328125\n",
      "val/f1: 0.46674108505249023\n",
      "val/taskclf_loss: 1.106689453125\n",
      "val/loss: 1.284916639328003\n",
      "val/mlm_loss: 1.9042987823486328\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f042adb1f5840a6a957c114745ff9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5149205d18804f9b96064cc68e6562d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.7678799629211426\n",
      "val/f1: 0.7687401175498962\n",
      "val/taskclf_loss: 0.5603954195976257\n",
      "val/loss: 0.8338254690170288\n",
      "val/mlm_loss: 1.7840598821640015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e6dc3741a14ee9989cf483059f96de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.7791507244110107\n",
      "val/f1: 0.7797816395759583\n",
      "val/taskclf_loss: 0.542116641998291\n",
      "val/loss: 0.8219900727272034\n",
      "val/mlm_loss: 1.7946168184280396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e25421d275498fa19ac081599b940c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.7813058495521545\n",
      "val/f1: 0.7824388742446899\n",
      "val/taskclf_loss: 0.5601878762245178\n",
      "val/loss: 0.8340222835540771\n",
      "val/mlm_loss: 1.7856619358062744\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c13ffe4ac7547f49bd3e1c7dd461d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.7936223149299622\n",
      "val/f1: 0.7939064502716064\n",
      "val/taskclf_loss: 0.6001781821250916\n",
      "val/loss: 0.8598576188087463\n",
      "val/mlm_loss: 1.7623056173324585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb77f93a98b474eaed53a5f51669ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.7959291338920593\n",
      "val/f1: 0.7969070076942444\n",
      "val/taskclf_loss: 0.5770742893218994\n",
      "val/loss: 0.8401689529418945\n",
      "val/mlm_loss: 1.7544854879379272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2589b74bb2a41ffb80c736885468713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.7972461581230164\n",
      "val/f1: 0.7981223464012146\n",
      "val/taskclf_loss: 0.5748772621154785\n",
      "val/loss: 0.841346800327301\n",
      "val/mlm_loss: 1.7673918008804321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86c5d7b7e6b47438c9431af76b0af99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.7972461581230164\n",
      "val/f1: 0.798103928565979\n",
      "val/taskclf_loss: 0.5746821165084839\n",
      "val/loss: 0.8369223475456238\n",
      "val/mlm_loss: 1.7482695579528809\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee3d4638d7648b2b8b63427e68d2178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.7968869805335999\n",
      "val/f1: 0.7977375984191895\n",
      "val/taskclf_loss: 0.5744446516036987\n",
      "val/loss: 0.8408291935920715\n",
      "val/mlm_loss: 1.7665786743164062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b875eade924cde8145973831fb0e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.7967672348022461\n",
      "val/f1: 0.7976281046867371\n",
      "val/taskclf_loss: 0.574231743812561\n",
      "val/loss: 0.8423925638198853\n",
      "val/mlm_loss: 1.7743151187896729\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee7283fa0874da4a11cd67fb96be30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.7968869805335999\n",
      "val/f1: 0.79774409532547\n",
      "val/taskclf_loss: 0.5740534067153931\n",
      "val/loss: 0.8411875367164612\n",
      "val/mlm_loss: 1.7695424556732178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint path: ./lightning_logs/kpvoid2e/checkpoints/task-TESUni-epoch=01-val_loss=0.82.ckpt\n",
      "Saved epoch checkpoint path: ./lightning_logs/kpvoid2e/checkpoints/TESUni-epoch=05.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ca6fbba5914d52b0f1c6911f2162e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a16c3b87f8a4d5c8d4823d6d8be542e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227f991964f643eab3e74494dad4b858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ff7398b1f14bc7851ce23fd446a19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source genre: telephone\n",
      "Target genre: slate\n",
      "Number of target samples: 69575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af154b84dcff4713a14e6052db612678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa11471f2c64f43b1b51a814f515ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadd975a197d4b1085225da29cec3f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713cbb2f27c74ca5b7d2afe230210fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05840cd1637e42cd956efc6f9fa7e651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   source_test/accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8117079138755798     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      source_test/f1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8116697072982788     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   source_test/f1_macro    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8027613162994385     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     source_test/loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.573732852935791     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   target_test/accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7332949042320251     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      target_test/f1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7329106330871582     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   target_test/f1_macro    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.723328709602356     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     target_test/loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.759901762008667     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m  source_test/accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8117079138755798    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     source_test/f1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8116697072982788    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  source_test/f1_macro   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8027613162994385    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    source_test/loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.573732852935791    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  target_test/accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7332949042320251    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     target_test/f1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7329106330871582    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  target_test/f1_macro   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.723328709602356    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    target_test/loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.759901762008667    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results Last Epoch: [{'source_test/loss': 0.573732852935791, 'source_test/accuracy': 0.8117079138755798, 'source_test/f1': 0.8116697072982788, 'source_test/f1_macro': 0.8027613162994385, 'target_test/loss': 0.759901762008667, 'target_test/accuracy': 0.7332949042320251, 'target_test/f1': 0.7329106330871582, 'target_test/f1_macro': 0.723328709602356}]\n",
      "Best checkpoint path: ./lightning_logs/kpvoid2e/checkpoints/task-TESUni-epoch=01-val_loss=0.82.ckpt\n",
      "Saved epoch checkpoint path: ./lightning_logs/kpvoid2e/checkpoints/TESUni-epoch=05.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7d7f86587240a2814ea784b2bb6cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   source_test/accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7986031174659729     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      source_test/f1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7992354035377502     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   source_test/f1_macro    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7878866791725159     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     source_test/loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5176754593849182     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   target_test/accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.700532853603363     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      target_test/f1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6990686655044556     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   target_test/f1_macro    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6926193833351135     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     target_test/loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7109049558639526     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m  source_test/accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7986031174659729    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     source_test/f1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7992354035377502    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  source_test/f1_macro   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7878866791725159    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    source_test/loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5176754593849182    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  target_test/accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.700532853603363    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     target_test/f1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6990686655044556    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  target_test/f1_macro   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6926193833351135    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    target_test/loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7109049558639526    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results on Best Model: [{'source_test/loss': 0.5176754593849182, 'source_test/accuracy': 0.7986031174659729, 'source_test/f1': 0.7992354035377502, 'source_test/f1_macro': 0.7878866791725159, 'target_test/loss': 0.7109049558639526, 'target_test/accuracy': 0.700532853603363, 'target_test/f1': 0.6990686655044556, 'target_test/f1_macro': 0.6926193833351135}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8780d24ee6b945b38c18ce3acff6bddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   source_test/accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8122119903564453     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      source_test/f1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8121238350868225     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   source_test/f1_macro    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8033238649368286     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     source_test/loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5743333101272583     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   target_test/accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7327908873558044     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      target_test/f1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7324329018592834     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   target_test/f1_macro    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7226143479347229     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     target_test/loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.760591447353363     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m  source_test/accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8122119903564453    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     source_test/f1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8121238350868225    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  source_test/f1_macro   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8033238649368286    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    source_test/loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5743333101272583    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  target_test/accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7327908873558044    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     target_test/f1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7324329018592834    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  target_test/f1_macro   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7226143479347229    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    target_test/loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.760591447353363    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results on saved epoch: [{'source_test/loss': 0.5743333101272583, 'source_test/accuracy': 0.8122119903564453, 'source_test/f1': 0.8121238350868225, 'source_test/f1_macro': 0.8033238649368286, 'target_test/loss': 0.760591447353363, 'target_test/accuracy': 0.7327908873558044, 'target_test/f1': 0.7324329018592834, 'target_test/f1_macro': 0.7226143479347229}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.003 MB of 0.003 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.003 MB of 0.012 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.003 MB of 0.012 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.016 MB of 0.016 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                epoch ▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: source_test/accuracy █▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       source_test/f1 █▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: source_test/f1_macro █▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     source_test/loss █▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: target_test/accuracy █▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       target_test/f1 █▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: target_test/f1_macro █▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     target_test/loss █▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/accuracy ▂▁▃▃▃▅▄▄▄▄▅▇▄▆▆▇▅▇▆▆▅▆▆▇▇▆▆▇▆▇▇█▅█▇▇▅▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             train/f1 ▃▁▃▃▃▅▄▄▄▄▅▇▄▆▆▇▅▇▆▆▅▆▆▇▇▆▆▇▆▇▇█▅█▇▇▅▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss ▇█▆▆█▅▆▅▆██▄▄▄▄▂▅▄▄▂▅▃▃▂▃▃▄▂▄▂▃▁▅▁▁▁▆▃▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/mlm_loss ▃▆▄▄▄▄▄▄▇▄▆█▂▇▆▂▄▅▆▅▂▅▃▄▄▂▄▆▆▄▆▂▂▃▁▂▇▆▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/taskclf_loss ▇▇▆▆█▅▆▅▆█▇▃▅▄▄▂▅▄▄▂▆▃▃▂▃▃▄▂▃▂▃▂▅▂▂▂▆▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/accuracy ▁▄▄▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val/avg_accuracy ▁▄▄▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/avg_f1 ▁▄▄▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/avg_loss ▃▁▃█▄▅▄▄▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val/avg_mlm_loss ▆█▇▃▂▄▁▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val/avg_taskclf_loss ▃▁▃█▅▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val/f1 ▁▄▄▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val/loss ▃▁▃█▄▅▄▄▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/mlm_loss ▆█▇▃▂▄▁▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val/taskclf_loss ▃▁▃█▅▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_loss ▃▁▃█▄▅▄▄▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: source_test/accuracy 0.81221\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       source_test/f1 0.81212\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: source_test/f1_macro 0.80332\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     source_test/loss 0.57433\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: target_test/accuracy 0.73279\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       target_test/f1 0.73243\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: target_test/f1_macro 0.72261\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     target_test/loss 0.76059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/accuracy 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             train/f1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/loss 0.47254\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/mlm_loss 2.0325\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/taskclf_loss 0.02366\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  trainer/global_step 23450\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/accuracy 0.79688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val/avg_accuracy 0.79689\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/avg_f1 0.79774\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/avg_loss 0.84119\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val/avg_mlm_loss 1.76954\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val/avg_taskclf_loss 0.57405\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               val/f1 0.79774\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val/loss 0.84073\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/mlm_loss 1.76861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val/taskclf_loss 0.57373\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_loss 0.84119\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mTES_union_run_with_seed_42\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrawhani/mixed_edited/runs/kpvoid2e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrawhani/mixed_edited\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240830_161223-kpvoid2e/logs\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - Waiting for wandb.init()...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ Waiting for wandb.init()...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/guest/Desktop/projects/third-experiments/domain_adaptation_project/mixed/mixed/wandb/run-20240830_173031-yz9uampu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTES_union_run_with_seed_10\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrawhani/mixed_edited\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrawhani/mixed_edited/runs/yz9uampu\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source genre: telephone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target genre: slate\n",
      "Number of target samples: 69575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source genre: telephone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target genre: slate\n",
      "Number of target samples: 69575\n",
      "Source dataset length: 75013\n",
      "Target dataset length: 21585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/lightning_fabric/connector.py:563: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:257: Found unsupported keys in the lr scheduler dict: {'reduce_lr_on_plateau'}. HINT: remove them from the output of `configure_optimizers`.\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | model         | BertAdapterModel   | 119 M \n",
      "1 | criterion     | CrossEntropyLoss   | 0     \n",
      "2 | mlm_criterion | CrossEntropyLoss   | 0     \n",
      "3 | accuracy      | MulticlassAccuracy | 0     \n",
      "4 | f1            | MulticlassF1Score  | 0     \n",
      "5 | f1_macro      | MulticlassF1Score  | 0     \n",
      "6 | softmax       | Softmax            | 0     \n",
      "-----------------------------------------------------\n",
      "9.5 M     Trainable params\n",
      "109 M     Non-trainable params\n",
      "119 M     Total params\n",
      "476.013   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eee\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ee7af780b6497ca2f2ca600652313e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.234375\n",
      "val/f1: 0.25993409752845764\n",
      "val/taskclf_loss: 1.109283447265625\n",
      "val/loss: 1.2556959390640259\n",
      "val/mlm_loss: 1.7645139694213867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fa6aa22d2c4a40adc493defd384f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61645507096046bba9e368d5d1a11d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.7714559435844421\n",
      "val/f1: 0.7718485593795776\n",
      "val/taskclf_loss: 0.557565450668335\n",
      "val/loss: 0.8328756093978882\n",
      "val/mlm_loss: 1.7896440029144287\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879cb0bb04784f98b9c809053298fc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.7836845517158508\n",
      "val/f1: 0.7833713293075562\n",
      "val/taskclf_loss: 0.5341743230819702\n",
      "val/loss: 0.8132825493812561\n",
      "val/mlm_loss: 1.783250331878662\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392738beb3dd4370ac82398b6ec42c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.789311945438385\n",
      "val/f1: 0.7891321182250977\n",
      "val/taskclf_loss: 0.5501735806465149\n",
      "val/loss: 0.8294327855110168\n",
      "val/mlm_loss: 1.7999250888824463\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eab79da2170490c98676a16bfafc453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/accuracy: 0.7966474890708923\n",
      "val/f1: 0.7967719435691833\n",
      "val/taskclf_loss: 0.5899022817611694\n",
      "val/loss: 0.8553076386451721\n",
      "val/mlm_loss: 1.7776544094085693\n"
     ]
    }
   ],
   "source": [
    "reload(processed)\n",
    "for seed in seeds:\n",
    "    wandb.init(project=project_name, name=f'{domain}_{type}_run_with_seed_{seed}', config={'seed': seed})\n",
    "\n",
    "    try:\n",
    "        seed_everything(seed)\n",
    "\n",
    "        hparams = {\n",
    "            \"source_target\": \"telephone_slate\",\n",
    "            \"source_domain\": \"telephone\",\n",
    "            \"target_domain\": \"slate\",\n",
    "            \"domain_adapter_name\": \"mlm_union_S\",\n",
    "            \"task_adapter_name\": \"task_TESUni\",\n",
    "            \"pretrained_model_name\": \"bert-base-uncased\",\n",
    "            \"padding\": \"max_length\",\n",
    "            \"max_seq_length\": 128,\n",
    "            \"bsz\": 32,\n",
    "            \"num_classes\": 3,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"reduction_factor\": 16,\n",
    "            \"mode\": \"domain\",\n",
    "            \"saved_adapter_dir\": \"../../saved/adapters\",\n",
    "        }\n",
    "\n",
    "        save_dir = \"checkpoints\"\n",
    "        save_epoch_3 = 6  # Save model at the 3rd epoch\n",
    "        \n",
    "        dm = processed.DataModuleSourceTargetMixed(hparams)\n",
    "        dm.setup('fit')\n",
    "        dm.setup(\"test\")\n",
    "        source_length, target_length = dm.get_dataset_lengths()\n",
    "        print(f\"Source dataset length: {source_length}\")\n",
    "        print(f\"Target dataset length: {target_length}\")\n",
    "        model = JointDomainTaskAdapter(hparams,source_length,target_length)\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            filename=\"task-TESUni-{epoch:02d}-{val_loss:.2f}\",\n",
    "            save_top_k=1,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "        )\n",
    "        save_model_callback_epoch = ModelCheckpoint(\n",
    "            filename=\"TESUni-{epoch:02d}\",\n",
    "            every_n_epochs=save_epoch_3,\n",
    "            save_top_k=-1,\n",
    "        )\n",
    "\n",
    "        wandb_logger = WandbLogger()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing : {e}\")\n",
    "\n",
    "    try:\n",
    "        train_loader = dm.train_dataloader()\n",
    "        val_loader = dm.val_dataloader()\n",
    "        trainer = Trainer(\n",
    "            max_epochs=10,\n",
    "            accelerator=\"auto\",\n",
    "            precision=16,\n",
    "            \n",
    "            default_root_dir=\"checkpoints\",\n",
    "            logger=wandb_logger,\n",
    "            callbacks=[checkpoint_callback, save_model_callback_epoch],\n",
    "            limit_train_batches=1.0,\n",
    "            limit_val_batches=1.0,\n",
    "            limit_test_batches=1.0,\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        print(f\"Best checkpoint path: {checkpoint_callback.best_model_path}\")\n",
    "        print(f\"Saved epoch checkpoint path: {save_model_callback_epoch.best_model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training : {e}\")\n",
    "\n",
    "    try:\n",
    "        dm.setup(\"test\")\n",
    "        test_loader = dm.test_dataloader()\n",
    "        test_results_last = trainer.test(model, test_loader)\n",
    "        print(\"Test Results Last Epoch:\", test_results_last)\n",
    "\n",
    "        # Collect results for last epoch model\n",
    "        for key, value in test_results_last[0].items():\n",
    "            results[\"last_epoch\"][key].append(value)\n",
    "\n",
    "        # Paths to the saved checkpoints\n",
    "        best_checkpoint_path = checkpoint_callback.best_model_path\n",
    "        saved_epoch_checkpoint_path = save_model_callback_epoch.best_model_path\n",
    "        print(f\"Best checkpoint path: {best_checkpoint_path}\")\n",
    "        print(f\"Saved epoch checkpoint path: {saved_epoch_checkpoint_path}\")\n",
    "\n",
    "        best_model = JointDomainTaskAdapter.load_from_checkpoint(best_checkpoint_path,source_dataset_length=source_length, target_dataset_length=target_length)\n",
    "        test_results_best = trainer.test(best_model, test_loader)\n",
    "        print(\"Test Results on Best Model:\", test_results_best)\n",
    "        for key, value in test_results_best[0].items():\n",
    "            results[\"best_model\"][key].append(value)\n",
    "\n",
    "        saved_epoch_model = JointDomainTaskAdapter.load_from_checkpoint(saved_epoch_checkpoint_path,source_dataset_length=source_length, target_dataset_length=target_length)\n",
    "        test_results_saved_epoch = trainer.test(saved_epoch_model, test_loader)\n",
    "        print(\"Test Results on saved epoch:\", test_results_saved_epoch)\n",
    "        for key, value in test_results_saved_epoch[0].items():\n",
    "            results[\"epoch_saved\"][key].append(value)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {e}\")\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b54e1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f0584",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each scenario\n",
    "mean_results = {scenario: {key: np.mean(values) for key, values in metrics.items()} for scenario, metrics in results.items()}\n",
    "std_results = {scenario: {key: np.std(values) for key, values in metrics.items()} for scenario, metrics in results.items()}\n",
    "\n",
    "# Log mean and standard deviation results to wandb\n",
    "wandb.init(project=project_name, name=f'{domain}_mean_results')\n",
    "for scenario in mean_results:\n",
    "    for key, value in mean_results[scenario].items():\n",
    "        wandb.log({f\"{scenario}/{key}\": value})\n",
    "        wandb.log({f\"{scenario}/{key}_std\": std_results[scenario][key]})\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Mean Results:\", mean_results)\n",
    "print(\"Standard Deviation Results:\", std_results)\n",
    "\n",
    "# # Save the best model's adapter\n",
    "# if model:\n",
    "#     adapter_save_path = f\"../../saved/adapter_after_run/{hparams['task_adapter_name']}\"\n",
    "#     model.save_adapter(adapter_save_path, hparams['task_adapter_name'])\n",
    "#     print(f\"Adapter saved to {adapter_save_path}\")\n",
    "# else:\n",
    "#     print(\"No best model to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c11251",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('dones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3584fff4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10286447",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "TES_union_mixed.ipynb",
   "output_path": "TES_union_mixed.ipynb",
   "parameters": {},
   "start_time": "2024-08-30T13:12:17.270379",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}